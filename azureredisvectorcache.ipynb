{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure Cache for Redis Enterprise - Smeantic Caching**\n",
    "This workbook showcases the usage of Azure Cache for Redis enterprise as a smeantic caching layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing the required packages\n",
    "#This script is used to install the required packages for the project\n",
    "%pip install requests\n",
    "%pip install redis\n",
    "%pip install numpy\n",
    "%pip install python-dotenv\n",
    "%pip install redisvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_OPENAI_ENDPOINT:  https://skd-apim-test.azure-api.net/aigatewaylandingzonesample/\n",
      "https://skd-apim-test.azure-api.net/aigatewaylandingzonesample//openai/deployments/skd-openaichat-gpt35/chat/completions?api-version=2024-02-01\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import getpass\n",
    "import time\n",
    "import redis\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "\n",
    "# load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "# make sure you have a .env file in the same directory as this notebook with the following variables set:\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "API_VERSION=os.getenv(\"OPENAI_API_VERSION\")\n",
    "CHATCOMPLETION_DEPLOYMENT_NAME=os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "CHATCOMPLETION_MODEL_NAME=os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL_NAME\")\n",
    "EMBEDDINGS_DEPLOYMENT_NAME=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "EMBEDDINGS_MODEL_NAME=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\")\n",
    "  \n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_ID = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "url = RESOURCE_ENDPOINT + \"/openai/deployments/\" + DEPLOYMENT_ID + \"/chat/completions?api-version=\" + API_VERSION\n",
    "\n",
    "#print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create redis url for creation of a new cache index\n",
    "REDIS_ENDPOINT = os.getenv(\"REDIS_ENDPOINT\")\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n",
    "\n",
    "if REDIS_ENDPOINT and REDIS_PASSWORD:\n",
    "\tredis_url = \"rediss://:\" + REDIS_PASSWORD + \"@\" + REDIS_ENDPOINT\n",
    "\t#print(\"Redis URL: \" + redis_url)\n",
    "else:\n",
    "\tprint(\"Error: REDIS_ENDPOINT or REDIS_PASSWORD environment variable is not set.\")\n",
    "\tredis_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satishka\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Create the semantic cache index, do not run this multiple times\n",
    "llmcache = SemanticCache(\n",
    "    name=\"llmcache-demo-cts1234\",                     # underlying search index name\n",
    "    redis_url=redis_url,                             # redis connection url string\n",
    "    distance_threshold=0.3,                          # semantic cache distance threshold\n",
    "    embedding_provider=\"azure_openai\",               # specify embedding provider as Azure OpenAI\n",
    "    embedding_config={\n",
    "        \"api_key\": API_KEY,\n",
    "        \"api_version\": API_VERSION,\n",
    "        \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"deployment_id\": EMBEDDINGS_DEPLOYMENT_NAME\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit, returning response from cache\n",
      "Prompt: What are the top 5 scientific breakthroughs of 19th century\n",
      "Response: [{'response': '1. Theory of Evolution by Natural Selection (Charles Darwin, 1859)\\n2. Discovery of the Electron (J.J. Thomson, 1897)\\n3. Germ Theory of Disease (Louis Pasteur, 1861)\\n4. Periodic Table of Elements (Dmitri Mendeleev, 1869)\\n5. Conservation of Energy (James Joule, 1843)', 'key': 'llmcache-demo-cts1234:c24005385aceff5e0083c0ca97aca4ec5631856dd468007144072fad7c65390b'}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Invoke Azure OpenAI Chat Completion API without semantic cache and note the response time.\n",
    "\n",
    "# Get user prompt\n",
    "userprompt = input(\"Enter your prompt: \")\n",
    "\n",
    "# Check if response exists in semantic cache\n",
    "start_time = time.time()\n",
    "if r := llmcache.check(prompt=userprompt, return_fields=[\"response\"]):\n",
    "    print(\"Cache hit, returning response from cache\")\n",
    "    print(f\"Prompt: {userprompt}\\nResponse: {r}\")\n",
    "else:\n",
    "    print(\"Empty cache, calling LLM to generate response\")\n",
    "    # Call Azure OpenAI API for new response\n",
    "    r = requests.post(\n",
    "        url, \n",
    "        headers={\"api-key\": API_KEY}, \n",
    "        json={\n",
    "            \"messages\":[\n",
    "                {\"role\": \"assistant\", \"content\": \"You are an AI assistant that helps people find information. \"}, \n",
    "                {\"role\": \"user\", \"content\": userprompt}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    # Extract the response\n",
    "    response_content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    # Store the response in semantic cache\n",
    "    print(\"Store the response from LLM in cache\")\n",
    "    llmcache.store(\n",
    "        prompt=userprompt,\n",
    "        response=response_content\n",
    "    )\n",
    "    # Display the response\n",
    "    print(f\"Prompt: {userprompt}\\nResponse: {response_content}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"\\nExecution time: {execution_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
