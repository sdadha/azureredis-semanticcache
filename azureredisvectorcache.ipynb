{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure Cache for Redis Enterprise - Smeantic Caching**\n",
    "This workbook showcases the usage of Azure Cache for Redis enterprise as a smeantic caching layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing the required packages\n",
    "#This script is used to install the required packages for the project\n",
    "%pip install requests\n",
    "%pip install redis\n",
    "%pip install numpy\n",
    "%pip install python-dotenv\n",
    "%pip install redisvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the environment, imports necessary libraries, and configures API credentials\n",
    "# for working with Azure OpenAI and Redis Enterprise cache\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import getpass\n",
    "import time\n",
    "import redis\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "\n",
    "# load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "# make sure you have a .env file in the same directory as this notebook with the following variables set:\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "API_VERSION=os.getenv(\"OPENAI_API_VERSION\")\n",
    "CHATCOMPLETION_DEPLOYMENT_NAME=os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "CHATCOMPLETION_MODEL_NAME=os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL_NAME\")\n",
    "EMBEDDINGS_DEPLOYMENT_NAME=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "EMBEDDINGS_MODEL_NAME=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\")\n",
    "  \n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_ID = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "url = RESOURCE_ENDPOINT + \"/openai/deployments/\" + DEPLOYMENT_ID + \"/chat/completions?api-version=\" + API_VERSION\n",
    "\n",
    "#print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create redis url for creation of a new cache index\n",
    "REDIS_ENDPOINT = os.getenv(\"REDIS_ENDPOINT\")\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n",
    "\n",
    "if REDIS_ENDPOINT and REDIS_PASSWORD:\n",
    "\tredis_url = \"rediss://:\" + REDIS_PASSWORD + \"@\" + REDIS_ENDPOINT\n",
    "\t#print(\"Redis URL: \" + redis_url)\n",
    "else:\n",
    "\tprint(\"Error: REDIS_ENDPOINT or REDIS_PASSWORD environment variable is not set.\")\n",
    "\tredis_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the semantic cache index, do not run this multiple times\n",
    "llmcache = SemanticCache(\n",
    "    name=\"llmcache-demo-cts1234\",                     # underlying search index name\n",
    "    redis_url=redis_url,                             # redis connection url string\n",
    "    distance_threshold=0.3,                          # semantic cache distance threshold\n",
    "    embedding_provider=\"azure_openai\",               # specify embedding provider as Azure OpenAI\n",
    "    embedding_config={\n",
    "        \"api_key\": API_KEY,\n",
    "        \"api_version\": API_VERSION,\n",
    "        \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"deployment_id\": EMBEDDINGS_DEPLOYMENT_NAME\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Invoke Azure OpenAI Chat Completion API without semantic cache and note the response time.\n",
    "\n",
    "# Get user prompt\n",
    "userprompt = input(\"Enter your prompt: \")\n",
    "\n",
    "# Check if response exists in semantic cache\n",
    "start_time = time.time()\n",
    "if r := llmcache.check(prompt=userprompt, return_fields=[\"response\"]):\n",
    "    print(\"Cache hit, returning response from cache\")\n",
    "    print(f\"Prompt: {userprompt}\\nResponse: {r}\")\n",
    "else:\n",
    "    print(\"Empty cache, calling LLM to generate response\")\n",
    "    # Call Azure OpenAI API for new response\n",
    "    r = requests.post(\n",
    "        url, \n",
    "        headers={\"api-key\": API_KEY}, \n",
    "        json={\n",
    "            \"messages\":[\n",
    "                {\"role\": \"assistant\", \"content\": \"You are an AI assistant that helps people find information. \"}, \n",
    "                {\"role\": \"user\", \"content\": userprompt}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    # Extract the response\n",
    "    response_content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    # Store the response in semantic cache\n",
    "    print(\"Store the response from LLM in cache\")\n",
    "    llmcache.store(\n",
    "        prompt=userprompt,\n",
    "        response=response_content\n",
    "    )\n",
    "    # Display the response\n",
    "    print(f\"Prompt: {userprompt}\\nResponse: {response_content}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"\\nExecution time: {execution_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
